{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-23T11:34:36.843466Z",
     "iopub.status.busy": "2025-06-23T11:34:36.843163Z",
     "iopub.status.idle": "2025-06-23T11:34:36.852594Z",
     "shell.execute_reply": "2025-06-23T11:34:36.851669Z",
     "shell.execute_reply.started": "2025-06-23T11:34:36.843444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nasa-cmaps/CMaps/RUL_FD002.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/test_FD003.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/Damage Propagation Modeling.pdf\n",
      "/kaggle/input/nasa-cmaps/CMaps/readme.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/train_FD003.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/test_FD004.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/train_FD004.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/x.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/test_FD002.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/train_FD001.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/train_FD002.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/RUL_FD001.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/RUL_FD004.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/RUL_FD003.txt\n",
      "/kaggle/input/nasa-cmaps/CMaps/test_FD001.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/RUL_FD002.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/test_FD003.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/Damage Propagation Modeling.pdf\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/readme.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/train_FD003.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/test_FD004.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/train_FD004.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/x.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/test_FD002.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/train_FD001.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/train_FD002.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/RUL_FD001.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/RUL_FD004.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/RUL_FD003.txt\n",
      "/kaggle/input/nasa-cmaps/cmaps/CMaps/test_FD001.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:36:15.430636Z",
     "iopub.status.busy": "2025-06-23T11:36:15.430364Z",
     "iopub.status.idle": "2025-06-23T11:36:22.059303Z",
     "shell.execute_reply": "2025-06-23T11:36:22.058117Z",
     "shell.execute_reply.started": "2025-06-23T11:36:15.430617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FD001:\n",
      "Shape of X_train: (15731, 50, 16)\n",
      "Shape of y_train: (15731,)\n",
      "Shape of X_test: (100, 50, 16)\n",
      "Shape of y_test_true: (100,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep=r'\\s+', header=None)\n",
    "    df = df.iloc[:, :26] # Select only the first 26 columns\n",
    "\n",
    "    df.columns = [\"unit_number\", \"time_in_cycles\", \"op_setting_1\", \"op_setting_2\", \"op_setting_3\"] + \\\n",
    "                 [f\"sensor_measurement_{i}\" for i in range(1, 22)]\n",
    "    return df\n",
    "\n",
    "def calculate_rul(df):\n",
    "    max_cycles = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_number', 'max_cycles']\n",
    "    df = pd.merge(df, max_cycles, on='unit_number', how='left')\n",
    "    df['RUL'] = df['max_cycles'] - df['time_in_cycles']\n",
    "    \n",
    "    RUL_MAX = 125\n",
    "    df['RUL'] = df['RUL'].apply(lambda x: min(x, RUL_MAX))\n",
    "    \n",
    "    df.drop(columns=['max_cycles'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def select_features(df):\n",
    "    drop_sensors = [f'sensor_measurement_{i}' for i in [1, 5, 6, 10, 16, 18, 19]]\n",
    "    drop_settings = ['op_setting_3']\n",
    "    \n",
    "    features_to_keep = [col for col in df.columns if col not in drop_sensors + drop_settings]\n",
    "    return df[features_to_keep]\n",
    "\n",
    "def create_sequences(df, sequence_length, sensor_cols, op_setting_cols):\n",
    "    X, y = [], []\n",
    "    features = sensor_cols + op_setting_cols\n",
    "    \n",
    "    for unit_number in df['unit_number'].unique():\n",
    "        unit_df = df[df['unit_number'] == unit_number].copy()\n",
    "        unit_df = unit_df.sort_values(by='time_in_cycles')\n",
    "        \n",
    "        for i in range(len(unit_df) - sequence_length + 1):\n",
    "            X.append(unit_df[features].iloc[i:i+sequence_length].values)\n",
    "            y.append(unit_df['RUL'].iloc[i+sequence_length-1])\n",
    "            \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def preprocess_dataset(dataset_id, sequence_length=50, rul_max=125):\n",
    "\n",
    "    sensor_cols = [f\"sensor_measurement_{i}\" for i in range(1, 22) if i not in [1, 5, 6, 10, 16, 18, 19]]\n",
    "    op_setting_cols = [\"op_setting_1\", \"op_setting_2\"]\n",
    "    all_feature_cols = sensor_cols + op_setting_cols\n",
    "\n",
    "\n",
    "    train_file_path = f\"/kaggle/input/nasa-cmaps/cmaps/CMaps/train_{dataset_id}.txt\"\n",
    "    train_df = load_data(train_file_path)\n",
    "    train_df = calculate_rul(train_df)\n",
    "    train_df = select_features(train_df)\n",
    "\n",
    "    # Normalize training data\n",
    "    scaler = MinMaxScaler()\n",
    "    train_df[all_feature_cols] = scaler.fit_transform(train_df[all_feature_cols])\n",
    "\n",
    "    # Create sequences for training data\n",
    "    X_train, y_train = create_sequences(train_df, sequence_length, sensor_cols, op_setting_cols)\n",
    "\n",
    "    # Load test data\n",
    "    test_file_path = f\"/kaggle/input/nasa-cmaps/cmaps/CMaps/test_{dataset_id}.txt\"\n",
    "    test_df = load_data(test_file_path)\n",
    "    test_df = select_features(test_df)\n",
    "\n",
    "    # Normalize test data using the scaler fitted on training data\n",
    "    test_df[all_feature_cols] = scaler.transform(test_df[all_feature_cols])\n",
    "\n",
    "    # Create sequences for test data\n",
    "    X_test_list = []\n",
    "    for unit_number in test_df['unit_number'].unique():\n",
    "        unit_df = test_df[test_df['unit_number'] == unit_number].copy()\n",
    "        unit_df = unit_df.sort_values(by='time_in_cycles')\n",
    "        \n",
    "        if len(unit_df) >= sequence_length:\n",
    "            X_test_list.append(unit_df[all_feature_cols].iloc[-sequence_length:].values)\n",
    "        else:\n",
    "            padded_sequence = np.zeros((sequence_length, len(all_feature_cols)))\n",
    "            padded_sequence[-len(unit_df):] = unit_df[all_feature_cols].values\n",
    "            X_test_list.append(padded_sequence)\n",
    "\n",
    "    X_test = np.array(X_test_list)\n",
    "\n",
    "\n",
    "    rul_test_file_path = f\"/kaggle/input/nasa-cmaps/cmaps/CMaps/RUL_{dataset_id}.txt\"\n",
    "    y_test_true = pd.read_csv(rul_test_file_path, sep=r'\\s+', header=None)\n",
    "    y_test_true = y_test_true.iloc[:, 0].values\n",
    "    y_test_true = np.array([min(x, rul_max) for x in y_test_true]) # Apply RUL capping\n",
    "\n",
    "    return X_train, y_train, X_test, y_test_true,scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_id = \"FD001\"\n",
    "    SEQUENCE_LENGTH = 50\n",
    "    RUL_MAX = 125\n",
    "\n",
    "    X_train, y_train, X_test, y_test_true,scaler = preprocess_dataset(dataset_id, SEQUENCE_LENGTH, RUL_MAX)\n",
    "\n",
    "    print(f\"Processed {dataset_id}:\")\n",
    "    print(f\"Shape of X_train: {X_train.shape}\")\n",
    "    print(f\"Shape of y_train: {y_train.shape}\")\n",
    "    print(f\"Shape of X_test: {X_test.shape}\")\n",
    "    print(f\"Shape of y_test_true: {y_test_true.shape}\")\n",
    "\n",
    "    np.save(f\"X_train_{dataset_id}.npy\", X_train)\n",
    "    np.save(f\"y_train_{dataset_id}.npy\", y_train)\n",
    "    np.save(f\"X_test_{dataset_id}.npy\", X_test)\n",
    "    np.save(f\"y_test_true_{dataset_id}.npy\", y_test_true)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:36:41.762698Z",
     "iopub.status.busy": "2025-06-23T11:36:41.762404Z",
     "iopub.status.idle": "2025-06-23T11:36:41.770518Z",
     "shell.execute_reply": "2025-06-23T11:36:41.769542Z",
     "shell.execute_reply.started": "2025-06-23T11:36:41.762675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer_scaler.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, 'transformer_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:34:45.590773Z",
     "iopub.status.busy": "2025-06-23T11:34:45.590422Z",
     "iopub.status.idle": "2025-06-23T11:34:45.613458Z",
     "shell.execute_reply": "2025-06-23T11:34:45.612219Z",
     "shell.execute_reply.started": "2025-06-23T11:34:45.590740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GatedConvUnit(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size=3):\n",
    "        super(GatedConvUnit, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_dim, output_dim, kernel_size, padding=kernel_size // 2)\n",
    "        self.gate = nn.Conv1d(input_dim, output_dim, kernel_size, padding=kernel_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, input_dim)\n",
    "        # Conv1d expects (batch_size, input_dim, sequence_length)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        conv_out = self.conv(x)\n",
    "        gate_out = torch.sigmoid(self.gate(x))\n",
    "        \n",
    "        # Permute back to (batch_size, sequence_length, output_dim)\n",
    "        return (conv_out * gate_out).permute(0, 2, 1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dense = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Scaled dot-product \n",
    "        matmul_qk = torch.matmul(q, k.permute(0, 1, 3, 2)) \n",
    "        dk = torch.tensor(self.head_dim, dtype=torch.float32)\n",
    "        scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        return self.dense(output), attention_weights\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim, dff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, dff)\n",
    "        self.linear2 = nn.Linear(dff, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(embed_dim, dff)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask) # Self-attention\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output) # Add & Norm\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output) # Add & Norm\n",
    "\n",
    "        return out2\n",
    "\n",
    "class TransformerRUL(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_layers, num_heads, dff, rate=0.1, max_rul=125):\n",
    "        super(TransformerRUL, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_rul = max_rul\n",
    "\n",
    "        # Local Feature Extraction Layer\n",
    "        self.gcu = GatedConvUnit(input_dim, embed_dim)\n",
    "        self.linear_gcu = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.pos_encoding = self.positional_encoding(1000, embed_dim) # Assuming max sequence length of 1000\n",
    "\n",
    "        # Encoder Layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, dff, rate) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Regression Layer\n",
    "        self.regression_linear = nn.Linear(embed_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(torch.arange(position).unsqueeze(1), \n",
    "                                     torch.arange(d_model).unsqueeze(0), \n",
    "                                     d_model)\n",
    "        \n",
    "\n",
    "        angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        \n",
    "        angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = angle_rads.unsqueeze(0)\n",
    "        return pos_encoding\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch_size, sequence_length, input_dim)\n",
    "\n",
    "        x = self.gcu(x) # (batch_size, sequence_length, embed_dim)\n",
    "        x = self.linear_gcu(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        seq_len = x.size(1)\n",
    "        x += self.pos_encoding[:, :seq_len, :].to(x.device)\n",
    "\n",
    "        # Encoder Layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "\n",
    "        x = torch.mean(x, dim=1) # (batch_size, embed_dim)\n",
    "        \n",
    "        output = self.regression_linear(x)\n",
    "        output = self.sigmoid(output) * self.max_rul # Scale sigmoid output to max_rul\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T08:56:09.023512Z",
     "iopub.status.busy": "2025-06-23T08:56:09.023104Z",
     "iopub.status.idle": "2025-06-23T09:38:49.577534Z",
     "shell.execute_reply": "2025-06-23T09:38:49.576836Z",
     "shell.execute_reply.started": "2025-06-23T08:56:09.023485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Dataset: FD001 ---\n",
      "Starting model training...\n",
      "Epoch 1/100, Training Loss: 337.1564\n",
      "Epoch 2/100, Training Loss: 140.9892\n",
      "Epoch 3/100, Training Loss: 130.4279\n",
      "Epoch 4/100, Training Loss: 127.9151\n",
      "Epoch 5/100, Training Loss: 119.4798\n",
      "Epoch 6/100, Training Loss: 116.0045\n",
      "Epoch 7/100, Training Loss: 111.9500\n",
      "Epoch 8/100, Training Loss: 108.0742\n",
      "Epoch 9/100, Training Loss: 102.0355\n",
      "Epoch 10/100, Training Loss: 90.1225\n",
      "Epoch 11/100, Training Loss: 82.2942\n",
      "Epoch 12/100, Training Loss: 70.2651\n",
      "Epoch 13/100, Training Loss: 63.1576\n",
      "Epoch 14/100, Training Loss: 56.1699\n",
      "Epoch 15/100, Training Loss: 53.0923\n",
      "Epoch 16/100, Training Loss: 46.7204\n",
      "Epoch 17/100, Training Loss: 38.0857\n",
      "Epoch 18/100, Training Loss: 35.4311\n",
      "Epoch 19/100, Training Loss: 30.5769\n",
      "Epoch 20/100, Training Loss: 29.0136\n",
      "Epoch 21/100, Training Loss: 24.5454\n",
      "Epoch 22/100, Training Loss: 21.4744\n",
      "Epoch 23/100, Training Loss: 20.7225\n",
      "Epoch 24/100, Training Loss: 18.8738\n",
      "Epoch 25/100, Training Loss: 16.6623\n",
      "Epoch 26/100, Training Loss: 15.8694\n",
      "Epoch 27/100, Training Loss: 13.4489\n",
      "Epoch 28/100, Training Loss: 14.0085\n",
      "Epoch 29/100, Training Loss: 13.0035\n",
      "Epoch 30/100, Training Loss: 10.1051\n",
      "Epoch 31/100, Training Loss: 10.4943\n",
      "Epoch 32/100, Training Loss: 10.5557\n",
      "Epoch 33/100, Training Loss: 9.0598\n",
      "Epoch 34/100, Training Loss: 9.8569\n",
      "Epoch 35/100, Training Loss: 7.6293\n",
      "Epoch 36/100, Training Loss: 7.9920\n",
      "Epoch 37/100, Training Loss: 9.2363\n",
      "Epoch 38/100, Training Loss: 6.3465\n",
      "Epoch 39/100, Training Loss: 6.6178\n",
      "Epoch 40/100, Training Loss: 8.1924\n",
      "Epoch 41/100, Training Loss: 5.7607\n",
      "Epoch 42/100, Training Loss: 8.4096\n",
      "Epoch 43/100, Training Loss: 6.0260\n",
      "Epoch 44/100, Training Loss: 5.5546\n",
      "Epoch 45/100, Training Loss: 9.7922\n",
      "Epoch 46/100, Training Loss: 7.3306\n",
      "Epoch 47/100, Training Loss: 4.0463\n",
      "Epoch 48/100, Training Loss: 3.9730\n",
      "Epoch 49/100, Training Loss: 4.9764\n",
      "Epoch 50/100, Training Loss: 5.2291\n",
      "Epoch 51/100, Training Loss: 7.2720\n",
      "Epoch 52/100, Training Loss: 4.8760\n",
      "Epoch 53/100, Training Loss: 4.5718\n",
      "Epoch 54/100, Training Loss: 4.4222\n",
      "Epoch 55/100, Training Loss: 3.6534\n",
      "Epoch 56/100, Training Loss: 3.8183\n",
      "Epoch 57/100, Training Loss: 5.3448\n",
      "Epoch 58/100, Training Loss: 4.7081\n",
      "Epoch 59/100, Training Loss: 3.7162\n",
      "Epoch 60/100, Training Loss: 2.8803\n",
      "Epoch 61/100, Training Loss: 4.4677\n",
      "Epoch 62/100, Training Loss: 5.1566\n",
      "Epoch 63/100, Training Loss: 2.9861\n",
      "Epoch 64/100, Training Loss: 5.3166\n",
      "Epoch 65/100, Training Loss: 2.6861\n",
      "Epoch 66/100, Training Loss: 2.9541\n",
      "Epoch 67/100, Training Loss: 2.8030\n",
      "Epoch 68/100, Training Loss: 8.7337\n",
      "Epoch 69/100, Training Loss: 2.9471\n",
      "Epoch 70/100, Training Loss: 2.0501\n",
      "Epoch 71/100, Training Loss: 2.2082\n",
      "Epoch 72/100, Training Loss: 4.4816\n",
      "Epoch 73/100, Training Loss: 4.8691\n",
      "Epoch 74/100, Training Loss: 1.9030\n",
      "Epoch 75/100, Training Loss: 2.0118\n",
      "Epoch 76/100, Training Loss: 2.3171\n",
      "Epoch 77/100, Training Loss: 8.8384\n",
      "Epoch 78/100, Training Loss: 2.9980\n",
      "Epoch 79/100, Training Loss: 1.8833\n",
      "Epoch 80/100, Training Loss: 1.8173\n",
      "Epoch 81/100, Training Loss: 2.0705\n",
      "Epoch 82/100, Training Loss: 2.2889\n",
      "Epoch 83/100, Training Loss: 2.5479\n",
      "Epoch 84/100, Training Loss: 8.7257\n",
      "Epoch 85/100, Training Loss: 2.5502\n",
      "Epoch 86/100, Training Loss: 1.5936\n",
      "Epoch 87/100, Training Loss: 1.5590\n",
      "Epoch 88/100, Training Loss: 1.9163\n",
      "Epoch 89/100, Training Loss: 2.0996\n",
      "Epoch 90/100, Training Loss: 8.2310\n",
      "Epoch 91/100, Training Loss: 3.0678\n",
      "Epoch 92/100, Training Loss: 1.4811\n",
      "Epoch 93/100, Training Loss: 1.4545\n",
      "Epoch 94/100, Training Loss: 1.7385\n",
      "Epoch 95/100, Training Loss: 1.8059\n",
      "Epoch 96/100, Training Loss: 3.5416\n",
      "Epoch 97/100, Training Loss: 3.1422\n",
      "Epoch 98/100, Training Loss: 1.9524\n",
      "Epoch 99/100, Training Loss: 1.4836\n",
      "Epoch 100/100, Training Loss: 1.6999\n",
      "Training complete.\n",
      "Root Mean Squared Error (RMSE) for FD001: 15.0819\n",
      "C-MAPSS Score for FD001: 562.0964\n",
      "Predictions saved to rul_predictions_FD001.csv\n",
      "\n",
      "--- Processing Dataset: FD002 ---\n",
      "Starting model training...\n",
      "Epoch 1/100, Training Loss: 1436.1785\n",
      "Epoch 2/100, Training Loss: 531.9214\n",
      "Epoch 3/100, Training Loss: 378.9208\n",
      "Epoch 4/100, Training Loss: 307.9022\n",
      "Epoch 5/100, Training Loss: 274.3291\n",
      "Epoch 6/100, Training Loss: 247.7953\n",
      "Epoch 7/100, Training Loss: 242.6676\n",
      "Epoch 8/100, Training Loss: 236.1032\n",
      "Epoch 9/100, Training Loss: 234.8730\n",
      "Epoch 10/100, Training Loss: 222.9254\n",
      "Epoch 11/100, Training Loss: 224.9364\n",
      "Epoch 12/100, Training Loss: 218.0670\n",
      "Epoch 13/100, Training Loss: 207.2157\n",
      "Epoch 14/100, Training Loss: 202.6277\n",
      "Epoch 15/100, Training Loss: 200.8244\n",
      "Epoch 16/100, Training Loss: 193.0083\n",
      "Epoch 17/100, Training Loss: 186.0058\n",
      "Epoch 18/100, Training Loss: 176.7738\n",
      "Epoch 19/100, Training Loss: 170.1545\n",
      "Epoch 20/100, Training Loss: 163.7096\n",
      "Epoch 21/100, Training Loss: 152.2007\n",
      "Epoch 22/100, Training Loss: 144.1668\n",
      "Epoch 23/100, Training Loss: 138.1199\n",
      "Epoch 24/100, Training Loss: 136.0276\n",
      "Epoch 25/100, Training Loss: 122.8328\n",
      "Epoch 26/100, Training Loss: 120.7636\n",
      "Epoch 27/100, Training Loss: 109.8189\n",
      "Epoch 28/100, Training Loss: 103.1347\n",
      "Epoch 29/100, Training Loss: 98.3395\n",
      "Epoch 30/100, Training Loss: 93.1672\n",
      "Epoch 31/100, Training Loss: 84.3924\n",
      "Epoch 32/100, Training Loss: 84.0035\n",
      "Epoch 33/100, Training Loss: 75.7171\n",
      "Epoch 34/100, Training Loss: 73.9709\n",
      "Epoch 35/100, Training Loss: 70.9157\n",
      "Epoch 36/100, Training Loss: 66.1208\n",
      "Epoch 37/100, Training Loss: 60.1851\n",
      "Epoch 38/100, Training Loss: 59.2981\n",
      "Epoch 39/100, Training Loss: 58.7276\n",
      "Epoch 40/100, Training Loss: 54.9608\n",
      "Epoch 41/100, Training Loss: 53.1273\n",
      "Epoch 42/100, Training Loss: 53.8042\n",
      "Epoch 43/100, Training Loss: 50.0385\n",
      "Epoch 44/100, Training Loss: 48.3801\n",
      "Epoch 45/100, Training Loss: 45.0835\n",
      "Epoch 46/100, Training Loss: 43.2346\n",
      "Epoch 47/100, Training Loss: 44.6242\n",
      "Epoch 48/100, Training Loss: 41.2555\n",
      "Epoch 49/100, Training Loss: 42.5628\n",
      "Epoch 50/100, Training Loss: 36.0293\n",
      "Epoch 51/100, Training Loss: 38.2306\n",
      "Epoch 52/100, Training Loss: 35.9102\n",
      "Epoch 53/100, Training Loss: 33.7860\n",
      "Epoch 54/100, Training Loss: 37.3238\n",
      "Epoch 55/100, Training Loss: 29.7801\n",
      "Epoch 56/100, Training Loss: 34.2829\n",
      "Epoch 57/100, Training Loss: 30.9944\n",
      "Epoch 58/100, Training Loss: 29.6104\n",
      "Epoch 59/100, Training Loss: 31.8589\n",
      "Epoch 60/100, Training Loss: 27.8432\n",
      "Epoch 61/100, Training Loss: 27.9955\n",
      "Epoch 62/100, Training Loss: 29.1835\n",
      "Epoch 63/100, Training Loss: 25.8678\n",
      "Epoch 64/100, Training Loss: 30.2939\n",
      "Epoch 65/100, Training Loss: 24.3962\n",
      "Epoch 66/100, Training Loss: 29.7766\n",
      "Epoch 67/100, Training Loss: 24.2419\n",
      "Epoch 68/100, Training Loss: 23.6608\n",
      "Epoch 69/100, Training Loss: 27.3419\n",
      "Epoch 70/100, Training Loss: 29.5167\n",
      "Epoch 71/100, Training Loss: 18.8781\n",
      "Epoch 72/100, Training Loss: 22.5182\n",
      "Epoch 73/100, Training Loss: 23.1938\n",
      "Epoch 74/100, Training Loss: 20.7691\n",
      "Epoch 75/100, Training Loss: 23.8042\n",
      "Epoch 76/100, Training Loss: 25.4157\n",
      "Epoch 77/100, Training Loss: 16.9434\n",
      "Epoch 78/100, Training Loss: 29.2500\n",
      "Epoch 79/100, Training Loss: 26.1632\n",
      "Epoch 80/100, Training Loss: 15.6315\n",
      "Epoch 81/100, Training Loss: 23.8138\n",
      "Epoch 82/100, Training Loss: 17.0737\n",
      "Epoch 83/100, Training Loss: 19.8800\n",
      "Epoch 84/100, Training Loss: 18.5670\n",
      "Epoch 85/100, Training Loss: 18.1862\n",
      "Epoch 86/100, Training Loss: 24.7541\n",
      "Epoch 87/100, Training Loss: 13.9976\n",
      "Epoch 88/100, Training Loss: 21.1670\n",
      "Epoch 89/100, Training Loss: 18.9214\n",
      "Epoch 90/100, Training Loss: 15.9020\n",
      "Epoch 91/100, Training Loss: 21.0824\n",
      "Epoch 92/100, Training Loss: 15.2612\n",
      "Epoch 93/100, Training Loss: 17.2102\n",
      "Epoch 94/100, Training Loss: 22.1351\n",
      "Epoch 95/100, Training Loss: 12.6082\n",
      "Epoch 96/100, Training Loss: 25.8387\n",
      "Epoch 97/100, Training Loss: 10.4645\n",
      "Epoch 98/100, Training Loss: 23.8432\n",
      "Epoch 99/100, Training Loss: 11.7229\n",
      "Epoch 100/100, Training Loss: 18.2144\n",
      "Training complete.\n",
      "Root Mean Squared Error (RMSE) for FD002: 24.6073\n",
      "C-MAPSS Score for FD002: 7144.3044\n",
      "Predictions saved to rul_predictions_FD002.csv\n",
      "\n",
      "--- Processing Dataset: FD003 ---\n",
      "Starting model training...\n",
      "Epoch 1/100, Training Loss: 368.8612\n",
      "Epoch 2/100, Training Loss: 143.3010\n",
      "Epoch 3/100, Training Loss: 133.8335\n",
      "Epoch 4/100, Training Loss: 120.4451\n",
      "Epoch 5/100, Training Loss: 111.1836\n",
      "Epoch 6/100, Training Loss: 110.0439\n",
      "Epoch 7/100, Training Loss: 105.3305\n",
      "Epoch 8/100, Training Loss: 102.6030\n",
      "Epoch 9/100, Training Loss: 99.7227\n",
      "Epoch 10/100, Training Loss: 96.2065\n",
      "Epoch 11/100, Training Loss: 95.8843\n",
      "Epoch 12/100, Training Loss: 93.7370\n",
      "Epoch 13/100, Training Loss: 89.8344\n",
      "Epoch 14/100, Training Loss: 87.7617\n",
      "Epoch 15/100, Training Loss: 84.6696\n",
      "Epoch 16/100, Training Loss: 80.8725\n",
      "Epoch 17/100, Training Loss: 77.2984\n",
      "Epoch 18/100, Training Loss: 73.9112\n",
      "Epoch 19/100, Training Loss: 69.7328\n",
      "Epoch 20/100, Training Loss: 65.8947\n",
      "Epoch 21/100, Training Loss: 62.1857\n",
      "Epoch 22/100, Training Loss: 56.3394\n",
      "Epoch 23/100, Training Loss: 57.5639\n",
      "Epoch 24/100, Training Loss: 55.1047\n",
      "Epoch 25/100, Training Loss: 50.8512\n",
      "Epoch 26/100, Training Loss: 49.7587\n",
      "Epoch 27/100, Training Loss: 44.8380\n",
      "Epoch 28/100, Training Loss: 44.1967\n",
      "Epoch 29/100, Training Loss: 39.7682\n",
      "Epoch 30/100, Training Loss: 39.6024\n",
      "Epoch 31/100, Training Loss: 35.5771\n",
      "Epoch 32/100, Training Loss: 35.6051\n",
      "Epoch 33/100, Training Loss: 32.8070\n",
      "Epoch 34/100, Training Loss: 31.9727\n",
      "Epoch 35/100, Training Loss: 31.6266\n",
      "Epoch 36/100, Training Loss: 25.6893\n",
      "Epoch 37/100, Training Loss: 26.6564\n",
      "Epoch 38/100, Training Loss: 27.4544\n",
      "Epoch 39/100, Training Loss: 22.8155\n",
      "Epoch 40/100, Training Loss: 26.1115\n",
      "Epoch 41/100, Training Loss: 21.3202\n",
      "Epoch 42/100, Training Loss: 19.8011\n",
      "Epoch 43/100, Training Loss: 19.7542\n",
      "Epoch 44/100, Training Loss: 22.1372\n",
      "Epoch 45/100, Training Loss: 18.5739\n",
      "Epoch 46/100, Training Loss: 15.5173\n",
      "Epoch 47/100, Training Loss: 19.8512\n",
      "Epoch 48/100, Training Loss: 15.9755\n",
      "Epoch 49/100, Training Loss: 13.6946\n",
      "Epoch 50/100, Training Loss: 17.3198\n",
      "Epoch 51/100, Training Loss: 15.6221\n",
      "Epoch 52/100, Training Loss: 11.9323\n",
      "Epoch 53/100, Training Loss: 11.6654\n",
      "Epoch 54/100, Training Loss: 11.7542\n",
      "Epoch 55/100, Training Loss: 14.6223\n",
      "Epoch 56/100, Training Loss: 10.4687\n",
      "Epoch 57/100, Training Loss: 11.6274\n",
      "Epoch 58/100, Training Loss: 9.1181\n",
      "Epoch 59/100, Training Loss: 12.9747\n",
      "Epoch 60/100, Training Loss: 7.9979\n",
      "Epoch 61/100, Training Loss: 10.0270\n",
      "Epoch 62/100, Training Loss: 8.5227\n",
      "Epoch 63/100, Training Loss: 8.7786\n",
      "Epoch 64/100, Training Loss: 9.5929\n",
      "Epoch 65/100, Training Loss: 10.2888\n",
      "Epoch 66/100, Training Loss: 7.9096\n",
      "Epoch 67/100, Training Loss: 5.6999\n",
      "Epoch 68/100, Training Loss: 5.7569\n",
      "Epoch 69/100, Training Loss: 18.5908\n",
      "Epoch 70/100, Training Loss: 5.7825\n",
      "Epoch 71/100, Training Loss: 5.1193\n",
      "Epoch 72/100, Training Loss: 5.2302\n",
      "Epoch 73/100, Training Loss: 5.4517\n",
      "Epoch 74/100, Training Loss: 15.3574\n",
      "Epoch 75/100, Training Loss: 4.7540\n",
      "Epoch 76/100, Training Loss: 4.0375\n",
      "Epoch 77/100, Training Loss: 4.8093\n",
      "Epoch 78/100, Training Loss: 14.7807\n",
      "Epoch 79/100, Training Loss: 5.3397\n",
      "Epoch 80/100, Training Loss: 4.2279\n",
      "Epoch 81/100, Training Loss: 3.8772\n",
      "Epoch 82/100, Training Loss: 4.5050\n",
      "Epoch 83/100, Training Loss: 18.0203\n",
      "Epoch 84/100, Training Loss: 6.1547\n",
      "Epoch 85/100, Training Loss: 3.4347\n",
      "Epoch 86/100, Training Loss: 3.3228\n",
      "Epoch 87/100, Training Loss: 4.4471\n",
      "Epoch 88/100, Training Loss: 3.5281\n",
      "Epoch 89/100, Training Loss: 3.9948\n",
      "Epoch 90/100, Training Loss: 15.8121\n",
      "Epoch 91/100, Training Loss: 4.7203\n",
      "Epoch 92/100, Training Loss: 3.0380\n",
      "Epoch 93/100, Training Loss: 3.0081\n",
      "Epoch 94/100, Training Loss: 3.1000\n",
      "Epoch 95/100, Training Loss: 3.7421\n",
      "Epoch 96/100, Training Loss: 3.9361\n",
      "Epoch 97/100, Training Loss: 4.0557\n",
      "Epoch 98/100, Training Loss: 15.5865\n",
      "Epoch 99/100, Training Loss: 3.1580\n",
      "Epoch 100/100, Training Loss: 2.4983\n",
      "Training complete.\n",
      "Root Mean Squared Error (RMSE) for FD003: 13.4029\n",
      "C-MAPSS Score for FD003: 364.1741\n",
      "Predictions saved to rul_predictions_FD003.csv\n",
      "\n",
      "--- Processing Dataset: FD004 ---\n",
      "Starting model training...\n",
      "Epoch 1/100, Training Loss: 965.8808\n",
      "Epoch 2/100, Training Loss: 366.6870\n",
      "Epoch 3/100, Training Loss: 317.6635\n",
      "Epoch 4/100, Training Loss: 298.3641\n",
      "Epoch 5/100, Training Loss: 288.6598\n",
      "Epoch 6/100, Training Loss: 271.1190\n",
      "Epoch 7/100, Training Loss: 275.7526\n",
      "Epoch 8/100, Training Loss: 259.7096\n",
      "Epoch 9/100, Training Loss: 258.9606\n",
      "Epoch 10/100, Training Loss: 253.8945\n",
      "Epoch 11/100, Training Loss: 252.4966\n",
      "Epoch 12/100, Training Loss: 240.4124\n",
      "Epoch 13/100, Training Loss: 243.0754\n",
      "Epoch 14/100, Training Loss: 234.4856\n",
      "Epoch 15/100, Training Loss: 234.9946\n",
      "Epoch 16/100, Training Loss: 231.8632\n",
      "Epoch 17/100, Training Loss: 227.1017\n",
      "Epoch 18/100, Training Loss: 223.6566\n",
      "Epoch 19/100, Training Loss: 216.5762\n",
      "Epoch 20/100, Training Loss: 216.6250\n",
      "Epoch 21/100, Training Loss: 203.1631\n",
      "Epoch 22/100, Training Loss: 196.2521\n",
      "Epoch 23/100, Training Loss: 188.2565\n",
      "Epoch 24/100, Training Loss: 183.1455\n",
      "Epoch 25/100, Training Loss: 175.4529\n",
      "Epoch 26/100, Training Loss: 164.1252\n",
      "Epoch 27/100, Training Loss: 161.1519\n",
      "Epoch 28/100, Training Loss: 148.8246\n",
      "Epoch 29/100, Training Loss: 141.2287\n",
      "Epoch 30/100, Training Loss: 137.6096\n",
      "Epoch 31/100, Training Loss: 129.5767\n",
      "Epoch 32/100, Training Loss: 124.1789\n",
      "Epoch 33/100, Training Loss: 113.6592\n",
      "Epoch 34/100, Training Loss: 110.0580\n",
      "Epoch 35/100, Training Loss: 102.8123\n",
      "Epoch 36/100, Training Loss: 101.1715\n",
      "Epoch 37/100, Training Loss: 90.1905\n",
      "Epoch 38/100, Training Loss: 83.7103\n",
      "Epoch 39/100, Training Loss: 80.1406\n",
      "Epoch 40/100, Training Loss: 73.9258\n",
      "Epoch 41/100, Training Loss: 73.5781\n",
      "Epoch 42/100, Training Loss: 70.6988\n",
      "Epoch 43/100, Training Loss: 62.0756\n",
      "Epoch 44/100, Training Loss: 60.2450\n",
      "Epoch 45/100, Training Loss: 56.9492\n",
      "Epoch 46/100, Training Loss: 51.1901\n",
      "Epoch 47/100, Training Loss: 54.9584\n",
      "Epoch 48/100, Training Loss: 55.0129\n",
      "Epoch 49/100, Training Loss: 41.0959\n",
      "Epoch 50/100, Training Loss: 44.6015\n",
      "Epoch 51/100, Training Loss: 42.0118\n",
      "Epoch 52/100, Training Loss: 49.8497\n",
      "Epoch 53/100, Training Loss: 36.5016\n",
      "Epoch 54/100, Training Loss: 44.8346\n",
      "Epoch 55/100, Training Loss: 32.3165\n",
      "Epoch 56/100, Training Loss: 32.9315\n",
      "Epoch 57/100, Training Loss: 33.6161\n",
      "Epoch 58/100, Training Loss: 36.3060\n",
      "Epoch 59/100, Training Loss: 28.5942\n",
      "Epoch 60/100, Training Loss: 38.5348\n",
      "Epoch 61/100, Training Loss: 24.8456\n",
      "Epoch 62/100, Training Loss: 29.6416\n",
      "Epoch 63/100, Training Loss: 24.7848\n",
      "Epoch 64/100, Training Loss: 31.9429\n",
      "Epoch 65/100, Training Loss: 22.1574\n",
      "Epoch 66/100, Training Loss: 27.7603\n",
      "Epoch 67/100, Training Loss: 21.6053\n",
      "Epoch 68/100, Training Loss: 24.2164\n",
      "Epoch 69/100, Training Loss: 25.3452\n",
      "Epoch 70/100, Training Loss: 36.4608\n",
      "Epoch 71/100, Training Loss: 17.2395\n",
      "Epoch 72/100, Training Loss: 19.4240\n",
      "Epoch 73/100, Training Loss: 25.7295\n",
      "Epoch 74/100, Training Loss: 14.9680\n",
      "Epoch 75/100, Training Loss: 36.7777\n",
      "Epoch 76/100, Training Loss: 13.5189\n",
      "Epoch 77/100, Training Loss: 24.4240\n",
      "Epoch 78/100, Training Loss: 18.0628\n",
      "Epoch 79/100, Training Loss: 26.3564\n",
      "Epoch 80/100, Training Loss: 13.5321\n",
      "Epoch 81/100, Training Loss: 19.6774\n",
      "Epoch 82/100, Training Loss: 20.0848\n",
      "Epoch 83/100, Training Loss: 17.5516\n",
      "Epoch 84/100, Training Loss: 18.4982\n",
      "Epoch 85/100, Training Loss: 14.6903\n",
      "Epoch 86/100, Training Loss: 32.0984\n",
      "Early stopping triggered after 86 epochs for FD004.\n",
      "Training complete.\n",
      "Root Mean Squared Error (RMSE) for FD004: 18.1829\n",
      "C-MAPSS Score for FD004: 1958.7638\n",
      "Predictions saved to rul_predictions_FD004.csv\n",
      "\n",
      "--- Overall Results ---\n",
      "Dataset FD001: RMSE = 15.0819, C-MAPSS Score = 562.0964\n",
      "Dataset FD002: RMSE = 24.6073, C-MAPSS Score = 7144.3044\n",
      "Dataset FD003: RMSE = 13.4029, C-MAPSS Score = 364.1741\n",
      "Dataset FD004: RMSE = 18.1829, C-MAPSS Score = 1958.7638\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "SEQUENCE_LENGTH = 50\n",
    "MAX_RUL = 125\n",
    "embed_dim = 64\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "dff = 128\n",
    "rate = 0.1\n",
    "\n",
    "num_epochs = 100 \n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "PATIENCE = 10\n",
    "min_delta = 0.001\n",
    "\n",
    "def train_and_evaluate_dataset(dataset_id):\n",
    "    print(f\"\\n--- Processing Dataset: {dataset_id} ---\")\n",
    "    X_train, y_train, X_test, y_test_true = preprocess_dataset(dataset_id, SEQUENCE_LENGTH, MAX_RUL)\n",
    "\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    input_dim = X_train.shape[2] # Number of features\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerRUL(input_dim, embed_dim, num_layers, num_heads, dff, rate, MAX_RUL).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "        if avg_train_loss + min_delta < best_loss:\n",
    "            best_loss = avg_train_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), f\"transformer_rul_model_{dataset_id}.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs for {dataset_id}.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "\n",
    "    model.load_state_dict(torch.load(f\"transformer_rul_model_{dataset_id}.pth\", map_location=device))\n",
    "    model.eval() \n",
    "\n",
    "    # Evaluation on test set\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_true_tensor = torch.tensor(y_test_true, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    actuals = y_test_true_tensor.cpu().numpy().flatten()\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(nn.MSELoss()(torch.tensor(predictions), torch.tensor(actuals)).item())\n",
    "    print(f\"Root Mean Squared Error (RMSE) for {dataset_id}: {rmse:.4f}\")\n",
    "\n",
    "    def calculate_cmapss_score(y_true, y_pred):\n",
    "        d = y_pred - y_true\n",
    "        score = 0\n",
    "        for val in d:\n",
    "            if val < 0:\n",
    "                score += (np.exp(-val/13) - 1)\n",
    "            else:\n",
    "                score += (np.exp(val/10) - 1)\n",
    "        return score\n",
    "\n",
    "    cmapss_score = calculate_cmapss_score(actuals, predictions)\n",
    "    print(f\"C-MAPSS Score for {dataset_id}: {cmapss_score:.4f}\")\n",
    "\n",
    "    pd.DataFrame({\"Actual_RUL\": actuals, \"Predicted_RUL\": predictions}).to_csv(f\"rul_predictions_{dataset_id}.csv\", index=False)\n",
    "    print(f\"Predictions saved to rul_predictions_{dataset_id}.csv\")\n",
    "\n",
    "    return rmse, cmapss_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_ids = [\"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
    "    results = {}\n",
    "\n",
    "    for dataset_id in dataset_ids:\n",
    "        rmse, cmapss_score = train_and_evaluate_dataset(dataset_id)\n",
    "        results[dataset_id] = {\"RMSE\": rmse, \"C-MAPSS Score\": cmapss_score}\n",
    "\n",
    "    print(\"\\n--- Overall Results ---\")\n",
    "    for dataset_id, metrics in results.items():\n",
    "        print(f\"Dataset {dataset_id}: RMSE = {metrics['RMSE']:.4f}, C-MAPSS Score = {metrics['C-MAPSS Score']:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T09:38:49.578869Z",
     "iopub.status.busy": "2025-06-23T09:38:49.578568Z",
     "iopub.status.idle": "2025-06-23T09:38:49.720416Z",
     "shell.execute_reply": "2025-06-23T09:38:49.719607Z",
     "shell.execute_reply.started": "2025-06-23T09:38:49.578844Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model evaluation...\n",
      "Root Mean Squared Error (RMSE): 15.0819\n",
      "C-MAPSS Score: 562.0964\n",
      "Predictions saved to rul_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_test = np.load(\"/kaggle/working/X_test_FD001.npy\")\n",
    "y_test_true = np.load(\"/kaggle/working/y_test_true_FD001.npy\")\n",
    "\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_true_tensor = torch.tensor(y_test_true, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_true_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_dim = X_test.shape[2]\n",
    "embed_dim = 64\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "dff = 128\n",
    "rate = 0.1 \n",
    "MAX_RUL = 125\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerRUL(input_dim, embed_dim, num_layers, num_heads, dff, rate, MAX_RUL).to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/transformer_rul_model_FD001.pth\", map_location=device))\n",
    "model.eval() \n",
    "\n",
    "print(\"Starting model evaluation...\")\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        predictions.extend(outputs.cpu().numpy().flatten())\n",
    "        actuals.extend(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "def calculate_cmapss_score(y_true, y_pred):\n",
    "    d = y_pred - y_true\n",
    "    score = 0\n",
    "    for val in d:\n",
    "        if val < 0:\n",
    "            score += (np.exp(-val/13) - 1)\n",
    "        else:\n",
    "            score += (np.exp(val/10) - 1)\n",
    "    return score\n",
    "\n",
    "score = calculate_cmapss_score(actuals, predictions)\n",
    "print(f\"C-MAPSS Score: {score:.4f}\")\n",
    "\n",
    "pd.DataFrame({\"Actual_RUL\": actuals, \"Predicted_RUL\": predictions}).to_csv(\"rul_predictions.csv\", index=False)\n",
    "print(\"Predictions saved to rul_predictions.csv\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 276801,
     "sourceId": 572434,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
